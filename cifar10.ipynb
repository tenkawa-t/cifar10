{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"cifar10.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true,"mount_file_id":"1O4s3TBPLpv73iB1KvNkcixJFt5RnzKjj","authorship_tag":"ABX9TyN4e00ubmRfUPi7RH7Mja+e"},"kernelspec":{"name":"python3","display_name":"Python 3"},"widgets":{"application/vnd.jupyter.widget-state+json":{"592bd661774d4d028286605405ab5574":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_2c5e4e8b695447beb453d8e9f658f641","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_d1c1f376d62549089ac5598b2b0120ae","IPY_MODEL_39f6a004dfd24ed298597d07ab8c59c5"]}},"2c5e4e8b695447beb453d8e9f658f641":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"d1c1f376d62549089ac5598b2b0120ae":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_616c4079e1ca4808b82d8e0bc203190a","_dom_classes":[],"description":"","_model_name":"FloatProgressModel","bar_style":"info","max":1,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":1,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_a7cd426a1df14aec80464ef9bfecab05"}},"39f6a004dfd24ed298597d07ab8c59c5":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_e81ced87fa114d4293c191e3d81eb9ba","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 170500096/? [00:20&lt;00:00, 32778283.25it/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_0399108bcac246fcb04f648cfecdf1b3"}},"616c4079e1ca4808b82d8e0bc203190a":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"a7cd426a1df14aec80464ef9bfecab05":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"e81ced87fa114d4293c191e3d81eb9ba":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"0399108bcac246fcb04f648cfecdf1b3":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}}}}},"cells":[{"cell_type":"code","metadata":{"id":"yh49D9_bAJTb","executionInfo":{"status":"ok","timestamp":1604809940299,"user_tz":-540,"elapsed":12748,"user":{"displayName":"Tenkawa Tomoyuki","photoUrl":"","userId":"08067809450853350491"}},"outputId":"3de47ea5-702f-4bec-bcff-c48a51f98155","colab":{"base_uri":"https://localhost:8080/"}},"source":["!pip install albumentations\n","!pip install torch\n","!pip install torchvision"],"execution_count":4,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: albumentations in /usr/local/lib/python3.6/dist-packages (0.1.12)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from albumentations) (1.4.1)\n","Requirement already satisfied: numpy>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from albumentations) (1.18.5)\n","Requirement already satisfied: opencv-python in /usr/local/lib/python3.6/dist-packages (from albumentations) (4.1.2.30)\n","Collecting imgaug<0.2.7,>=0.2.5\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ad/2e/748dbb7bb52ec8667098bae9b585f448569ae520031932687761165419a2/imgaug-0.2.6.tar.gz (631kB)\n","\u001b[K     |████████████████████████████████| 634kB 6.5MB/s \n","\u001b[?25hRequirement already satisfied: scikit-image>=0.11.0 in /usr/local/lib/python3.6/dist-packages (from imgaug<0.2.7,>=0.2.5->albumentations) (0.16.2)\n","Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from imgaug<0.2.7,>=0.2.5->albumentations) (1.15.0)\n","Requirement already satisfied: imageio>=2.3.0 in /usr/local/lib/python3.6/dist-packages (from scikit-image>=0.11.0->imgaug<0.2.7,>=0.2.5->albumentations) (2.4.1)\n","Requirement already satisfied: pillow>=4.3.0 in /usr/local/lib/python3.6/dist-packages (from scikit-image>=0.11.0->imgaug<0.2.7,>=0.2.5->albumentations) (7.0.0)\n","Requirement already satisfied: matplotlib!=3.0.0,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from scikit-image>=0.11.0->imgaug<0.2.7,>=0.2.5->albumentations) (3.2.2)\n","Requirement already satisfied: networkx>=2.0 in /usr/local/lib/python3.6/dist-packages (from scikit-image>=0.11.0->imgaug<0.2.7,>=0.2.5->albumentations) (2.5)\n","Requirement already satisfied: PyWavelets>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from scikit-image>=0.11.0->imgaug<0.2.7,>=0.2.5->albumentations) (1.1.1)\n","Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image>=0.11.0->imgaug<0.2.7,>=0.2.5->albumentations) (2.4.7)\n","Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image>=0.11.0->imgaug<0.2.7,>=0.2.5->albumentations) (2.8.1)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image>=0.11.0->imgaug<0.2.7,>=0.2.5->albumentations) (1.3.1)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image>=0.11.0->imgaug<0.2.7,>=0.2.5->albumentations) (0.10.0)\n","Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.6/dist-packages (from networkx>=2.0->scikit-image>=0.11.0->imgaug<0.2.7,>=0.2.5->albumentations) (4.4.2)\n","Building wheels for collected packages: imgaug\n","  Building wheel for imgaug (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for imgaug: filename=imgaug-0.2.6-cp36-none-any.whl size=654021 sha256=16d692115a7252b586658fd344806869d1cc6dca848b390fbdbc7bc4b1c4a02c\n","  Stored in directory: /root/.cache/pip/wheels/97/ec/48/0d25896c417b715af6236dbcef8f0bed136a1a5e52972fc6d0\n","Successfully built imgaug\n","Installing collected packages: imgaug\n","  Found existing installation: imgaug 0.2.9\n","    Uninstalling imgaug-0.2.9:\n","      Successfully uninstalled imgaug-0.2.9\n","Successfully installed imgaug-0.2.6\n","Requirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (1.7.0+cu101)\n","Requirement already satisfied: dataclasses in /usr/local/lib/python3.6/dist-packages (from torch) (0.7)\n","Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch) (0.16.0)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.6/dist-packages (from torch) (3.7.4.3)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torch) (1.18.5)\n","Requirement already satisfied: torchvision in /usr/local/lib/python3.6/dist-packages (0.8.1+cu101)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torchvision) (1.18.5)\n","Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.6/dist-packages (from torchvision) (7.0.0)\n","Requirement already satisfied: torch==1.7.0 in /usr/local/lib/python3.6/dist-packages (from torchvision) (1.7.0+cu101)\n","Requirement already satisfied: dataclasses in /usr/local/lib/python3.6/dist-packages (from torch==1.7.0->torchvision) (0.7)\n","Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch==1.7.0->torchvision) (0.16.0)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.6/dist-packages (from torch==1.7.0->torchvision) (3.7.4.3)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"boWcXKA-AU_k","executionInfo":{"status":"ok","timestamp":1604809942991,"user_tz":-540,"elapsed":12012,"user":{"displayName":"Tenkawa Tomoyuki","photoUrl":"","userId":"08067809450853350491"}}},"source":["#model.py\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import pandas as pd"],"execution_count":5,"outputs":[]},{"cell_type":"code","metadata":{"id":"QKdjUOB3A_nR","executionInfo":{"status":"ok","timestamp":1604809942993,"user_tz":-540,"elapsed":11649,"user":{"displayName":"Tenkawa Tomoyuki","photoUrl":"","userId":"08067809450853350491"}}},"source":["'''ResNet in PyTorch.\n","\n","For Pre-activation ResNet, see 'preact_resnet.py'.\n","\n","Reference:\n","[1] Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun\n","\tDeep Residual Learning for Image Recognition. arXiv:1512.03385\n","'''\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","\n","\n","class BasicBlock(nn.Module):\n","\texpansion = 1\n","\n","\tdef __init__(self, in_planes, planes, stride=1):\n","\t\tsuper(BasicBlock, self).__init__()\n","\t\tself.conv1 = nn.Conv2d(in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n","\t\tself.bn1 = nn.BatchNorm2d(planes)\n","\t\tself.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=1, padding=1, bias=False)\n","\t\tself.bn2 = nn.BatchNorm2d(planes)\n","\n","\t\tself.shortcut = nn.Sequential()\n","\t\tif stride != 1 or in_planes != self.expansion*planes:\n","\t\t\tself.shortcut = nn.Sequential(\n","\t\t\t\tnn.Conv2d(in_planes, self.expansion*planes, kernel_size=1, stride=stride, bias=False),\n","\t\t\t\tnn.BatchNorm2d(self.expansion*planes)\n","\t\t\t)\n","\n","\tdef forward(self, x):\n","\t\tout = F.relu(self.bn1(self.conv1(x)))\n","\t\tout = self.bn2(self.conv2(out))\n","\t\tout += self.shortcut(x)\n","\t\tout = F.relu(out)\n","\t\treturn out\n","\n","\n","class Bottleneck(nn.Module):\n","\texpansion = 4\n","\n","\tdef __init__(self, in_planes, planes, stride=1):\n","\t\tsuper(Bottleneck, self).__init__()\n","\t\tself.conv1 = nn.Conv2d(in_planes, planes, kernel_size=1, bias=False)\n","\t\tself.bn1 = nn.BatchNorm2d(planes)\n","\t\tself.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n","\t\tself.bn2 = nn.BatchNorm2d(planes)\n","\t\tself.conv3 = nn.Conv2d(planes, self.expansion*planes, kernel_size=1, bias=False)\n","\t\tself.bn3 = nn.BatchNorm2d(self.expansion*planes)\n","\n","\t\tself.shortcut = nn.Sequential()\n","\t\tif stride != 1 or in_planes != self.expansion*planes:\n","\t\t\tself.shortcut = nn.Sequential(\n","\t\t\t\tnn.Conv2d(in_planes, self.expansion*planes, kernel_size=1, stride=stride, bias=False),\n","\t\t\t\tnn.BatchNorm2d(self.expansion*planes)\n","\t\t\t)\n","\n","\tdef forward(self, x):\n","\t\tout = F.relu(self.bn1(self.conv1(x)))\n","\t\tout = F.relu(self.bn2(self.conv2(out)))\n","\t\tout = self.bn3(self.conv3(out))\n","\t\tout += self.shortcut(x)\n","\t\tout = F.relu(out)\n","\t\treturn out\n","\n","\n","class ResNet(nn.Module):\n","\tdef __init__(self, block, num_blocks, num_classes=10):\n","\t\tsuper(ResNet, self).__init__()\n","\t\tself.in_planes = 64\n","\n","\t\tself.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)\n","\t\tself.bn1 = nn.BatchNorm2d(64)\n","\t\tself.layer1 = self._make_layer(block, 64, num_blocks[0], stride=1)\n","\t\tself.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)\n","\t\tself.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)\n","\t\tself.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2)\n","\t\tself.linear = nn.Linear(512*block.expansion, num_classes)\n","\n","\tdef _make_layer(self, block, planes, num_blocks, stride):\n","\t\tstrides = [stride] + [1]*(num_blocks-1)\n","\t\tlayers = []\n","\t\tfor stride in strides:\n","\t\t\tlayers.append(block(self.in_planes, planes, stride))\n","\t\t\tself.in_planes = planes * block.expansion\n","\t\treturn nn.Sequential(*layers)\n","\n","\tdef forward(self, x):\n","\t\tout = F.relu(self.bn1(self.conv1(x)))\n","\t\tout = self.layer1(out)\n","\t\tout = self.layer2(out)\n","\t\tout = self.layer3(out)\n","\t\tout = self.layer4(out)\n","\t\tout = F.avg_pool2d(out, 4)\n","\t\tout = out.view(out.size(0), -1)\n","\t\tout = self.linear(out)\n","\t\treturn out\n","\n","\n","def ResNet18():\n","\treturn ResNet(BasicBlock, [2,2,2,2])\n","\n","def ResNet34():\n","\treturn ResNet(BasicBlock, [3,4,6,3])\n","\n","def ResNet50():\n","\treturn ResNet(Bottleneck, [3,4,6,3])\n","\n","def ResNet101():\n","\treturn ResNet(Bottleneck, [3,4,23,3])\n","\n","def ResNet152():\n","\treturn ResNet(Bottleneck, [3,8,36,3])\n","\n","\n","def test():\n","\tnet = ResNet18()\n","\ty = net(torch.randn(1,3,32,32))\n","\tprint(y.size())\n","\n","# test()\n"],"execution_count":6,"outputs":[]},{"cell_type":"code","metadata":{"id":"LyMdsJGNHkFf","executionInfo":{"status":"ok","timestamp":1604809945926,"user_tz":-540,"elapsed":2908,"user":{"displayName":"Tenkawa Tomoyuki","photoUrl":"","userId":"08067809450853350491"}}},"source":["'''Train CIFAR10 with PyTorch.'''\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import torch.nn.functional as F\n","import torch.backends.cudnn as cudnn\n","\n","import torchvision\n","import torchvision.transforms as transforms\n","import torch.backends.cudnn as cudnn\n","import torchvision\n","import torchvision.transforms as transforms\n","\n","import os\n","import argparse\n","from sklearn.metrics import classification_report\n","\n","import os\n","import argparse\n","\n","# data augment\n","# 訓練誤差\n","# lr 　optimizer 初期値　activation\n","# 過学習\n","# dataaugment validation\n","import cv2\n","import matplotlib.pyplot as plt\n","import albumentations as albu\n","from albumentations import (\n","    Compose, HorizontalFlip, CLAHE, HueSaturationValue,\n","    RandomBrightness, RandomContrast, RandomGamma,\n","    ToFloat, ShiftScaleRotate\n",")\n","import numpy as np\n","from PIL import Image\n","\n","\n","# def load_data(data_dir):\n","def load_data():\n","\n","\talbu_list = [\n","    HorizontalFlip(p=0.5),\n","    RandomContrast(limit=0.2, p=0.5),\n","    RandomGamma(gamma_limit=(80, 120), p=0.5),\n","    RandomBrightness(limit=0.2, p=0.5),\n","    HueSaturationValue(hue_shift_limit=5, sat_shift_limit=20,\n","                       val_shift_limit=10, p=.9),\n","    # CLAHE(p=1.0, clip_limit=2.0),\n","    ShiftScaleRotate(\n","        shift_limit=0.0625, scale_limit=0.1, \n","        rotate_limit=15, border_mode=cv2.BORDER_REFLECT_101, p=0.8)]\n","\n","\talbu_transforms = albu.Compose(albu_list)\n","\n","\tdef albumentations_transform(image, transform=albu_transforms):\n","\t\tif transform:\n","\t\t\timage_np = np.array(image)\n","\t\t\taugmented = transform(image=image_np)\n","\t\t\timage = Image.fromarray(augmented['image'])\n","\t\treturn image\n","\n","\ttransform_train = transforms.Compose([\n","\t\ttransforms.Lambda(albumentations_transform),\n","\t\ttransforms.ToTensor(),\n","\t\ttransforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n","\t])\n","\n","\ttransform_test = transforms.Compose([\n","\t\ttransforms.ToTensor(),\n","\t\ttransforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n","\t])\n","\ttrain_set = torchvision.datasets.CIFAR10(root='./data', train=True,\n","                                        download=True, transform=transform_train)\n","\ttrain_loader = torch.utils.data.DataLoader(train_set, batch_size=128,\n","                                          shuffle=True, num_workers=2)\n","\n","\ttest_set = torchvision.datasets.CIFAR10(root='./data', train=False,\n","                                       download=True, transform=transform_test)\n","\ttest_loader = torch.utils.data.DataLoader(test_set, batch_size=100,\n","                                         shuffle=False, num_workers=2)\n","\n","\t# train_set = torchvision.datasets.CIFAR10(root=data_dir, train=True, download=True, transform=transform_train)\n","\t# train_loader = torch.utils.data.DataLoader(train_set, batch_size=128, shuffle=True, num_workers=0)\n","\t# test_set = torchvision.datasets.CIFAR10(root=data_dir, train=False, download=True, transform=transform_test)\n","\t# test_loader = torch.utils.data.DataLoader(test_set, batch_size=100, shuffle=False, num_workers=0)\n","\tclass_names = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n","\n","\treturn train_loader, test_loader, class_names"],"execution_count":7,"outputs":[]},{"cell_type":"code","metadata":{"id":"XitXyIAKHzQ3","executionInfo":{"status":"ok","timestamp":1604809945928,"user_tz":-540,"elapsed":2900,"user":{"displayName":"Tenkawa Tomoyuki","photoUrl":"","userId":"08067809450853350491"}}},"source":["'''Train CIFAR10 with PyTorch.'''\n","\n","\n","def main():\n","    dataset_name = 'CIFAR10'\n","    model_name = 'ResNet18'\n","    model_ckpt_dir = '/content/drive/My Drive/cifar10/experiment/models/checkpoints'\n","    model_ckpt_path_temp = '/content/drive/My Drive/cifar10/experiment/models/checkpoints/02_100-200_{}_{}_epoch={}.pth'\n","    n_epoch = 100\n","    lr = 0.001\n","\n","    # Make directory.\n","    os.makedirs(model_ckpt_dir, exist_ok=True)\n","    # Validate paths.\n","    # Set device.\n","    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n","\n","    # Load dataset.\n","    train_loader, test_loader, class_names = load_data()\n","\n","    # Set a model.\n","    model = get_model(model_name)\n","    model_path = '/content/drive/My Drive/cifar10/experiment/models/checkpoints/CIFAR10_ResNet18_epoch=96.pth'\n","    model.load_state_dict(torch.load(model_path, map_location=torch.device('cpu')))\n","    model = model.to(device)\n","    \n","    # Set loss function and optimization function.\n","    criterion = nn.CrossEntropyLoss()\n","    # optimizer = optim.SGD(model.parameters(), lr=lr, momentum=0.9, weight_decay=5e-4)\n","    optimizer = optim.Adam(model.parameters(), lr=lr, betas=(0.9, 0.999), weight_decay=5e-4, eps=1e-08, amsgrad=False)\n","    # Train and test.\n","    list_test = []\n","    for epoch in range(n_epoch):\n","        # Train and test a model.\n","        train_acc, train_loss = train(model, device, train_loader, criterion, optimizer)\n","        test_acc, test_loss = test(model, device, test_loader, criterion)\n","    \n","        # Output score.\n","        stdout_temp = 'epoch: {:>3}, train acc: {:<8}, train loss: {:<8}, test acc: {:<8}, test loss: {:<8}'\n","        print(stdout_temp.format(epoch + 1, train_acc, train_loss, test_acc, test_loss))\n","        # Save a model checkpoint.\n","        model_ckpt_path = model_ckpt_path_temp.format(dataset_name, model_name, epoch + 1)\n","        torch.save(model.state_dict(), model_ckpt_path)\n","    \n","        print('Saved a model checkpoint at {}'.format(model_ckpt_path))\n","        print('')\n","        list_temp = []\n","        list_temp = [epoch + 1, model_ckpt_path, train_acc, train_loss, test_acc, test_loss]\n","        list_test.append(list_temp)\n","    df = pd.DataFrame(list_test).T\n","    df.colums = [\"epoch\", model_ckpt_path, \"train_acc\", \"train_loss\", \"test_acc\", \"test_loss\"]\n","    df.to_csv(f\"{model_ckpt_dir}_{model_name}_adam_test_100-200.csv\")\n","\n","\n","def train(model, device, train_loader, criterion, optimizer):\n","    model.train()\n","\n","    output_list = []\n","    target_list = []\n","    running_loss = 0.0\n","    for batch_idx, (inputs, targets) in enumerate(train_loader):\n","        # Forward processing.\n","        inputs, targets = inputs.to(device), targets.to(device)\n","        outputs = model(inputs)\n","        loss = criterion(outputs, targets)\n","\n","        # Backward processing.\n","        optimizer.zero_grad()\n","        loss.backward()\n","        optimizer.step()\n","\n","        # Set data to calculate score.\n","        output_list += [int(o.argmax()) for o in outputs]\n","        target_list += [int(t) for t in targets]\n","        running_loss += loss.item()\n","\n","        # Calculate score at present.\n","        train_acc, train_loss = calc_score(output_list, target_list, running_loss, train_loader)\n","        if batch_idx % 10 == 0 and batch_idx != 0:\n","            stdout_temp = 'batch: {:>3}/{:<3}, train acc: {:<8}, train loss: {:<8}'\n","            print(stdout_temp.format(batch_idx, len(train_loader), train_acc, train_loss))\n","\n","    # Calculate score.\n","    train_acc, train_loss = calc_score(output_list, target_list, running_loss, train_loader)\n","\n","    return train_acc, train_loss\n","\n","\n","def test(model, device, test_loader, criterion):\n","    model.eval()\n","\n","    output_list = []\n","    target_list = []\n","    running_loss = 0.0\n","    for batch_idx, (inputs, targets) in enumerate(test_loader):\n","        # Forward processing.\n","        inputs, targets = inputs.to(device), targets.to(device)\n","        outputs = model(inputs)\n","        loss = criterion(outputs, targets)\n","\n","        # Set data to calculate score.\n","        output_list += [int(o.argmax()) for o in outputs]\n","        target_list += [int(t) for t in targets]\n","        running_loss += loss.item()\n","\n","    test_acc, test_loss = calc_score(output_list, target_list, running_loss, test_loader)\n","\n","    return test_acc, test_loss\n","\n","\n","def get_model(model_name):\n","    if model_name == 'VGG19':\n","        model = VGG('VGG19')\n","    elif model_name == 'ResNet18':\n","        model = ResNet18()\n","    elif model_name == 'PreActResNet18':\n","        model = PreActResNet18()\n","    elif model_name == 'GoogLeNet':\n","        model = GoogLeNet()\n","    elif model_name == 'DenseNet121':\n","        model = DenseNet121()\n","    elif model_name == 'ResNeXt29_2x64d':\n","        model = ResNeXt29_2x64d()\n","    elif model_name == 'MobileNet':\n","        model = MobileNet()\n","    elif model_name == 'MobileNetV2':\n","        model = MobileNetV2()\n","    elif model_name == 'DPN92':\n","        model = DPN92()\n","    elif model_name == 'ShuffleNetG2':\n","        model = ShuffleNetG2()\n","    elif model_name == 'SENet18':\n","        model = SENet18()\n","    elif model_name == 'ShuffleNetV2':\n","        model = ShuffleNetV2(1)\n","    elif model_name == 'EfficientNetB0':\n","        model = EfficientNetB0()\n","    else:\n","        print('{} does NOT exist in repertory.'.format(model_name))\n","        sys.exit(1)\n","\n","    return model\n","\n","\n","def calc_score(output_list, target_list, running_loss, data_loader):\n","    # Calculate accuracy.\n","    result = classification_report(output_list, target_list, output_dict=True)\n","    acc = round(result['weighted avg']['f1-score'], 6)\n","    loss = round(running_loss / len(data_loader.dataset), 6)\n","\n","    return acc, loss\n","\n"],"execution_count":8,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"cPjXTWVYhkKN"},"source":["main()"]},{"cell_type":"code","metadata":{"id":"G1adyjPUuiNH","outputId":"97dce3dd-798e-47ee-dbdb-9cb8f0047c52","colab":{"base_uri":"https://localhost:8080/","height":1000,"referenced_widgets":["592bd661774d4d028286605405ab5574","2c5e4e8b695447beb453d8e9f658f641","d1c1f376d62549089ac5598b2b0120ae","39f6a004dfd24ed298597d07ab8c59c5","616c4079e1ca4808b82d8e0bc203190a","a7cd426a1df14aec80464ef9bfecab05","e81ced87fa114d4293c191e3d81eb9ba","0399108bcac246fcb04f648cfecdf1b3"]}},"source":["main()"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n"],"name":"stdout"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"592bd661774d4d028286605405ab5574","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["Extracting ./data/cifar-10-python.tar.gz to ./data\n","Files already downloaded and verified\n","batch:  10/391, train acc: 0.910768, train loss: 5.5e-05 \n","batch:  20/391, train acc: 0.909118, train loss: 0.000107\n","batch:  30/391, train acc: 0.906342, train loss: 0.000167\n","batch:  40/391, train acc: 0.905793, train loss: 0.000223\n","batch:  50/391, train acc: 0.903083, train loss: 0.000282\n","batch:  60/391, train acc: 0.904542, train loss: 0.000335\n","batch:  70/391, train acc: 0.90664 , train loss: 0.000383\n","batch:  80/391, train acc: 0.906144, train loss: 0.000437\n","batch:  90/391, train acc: 0.906228, train loss: 0.000493\n","batch: 100/391, train acc: 0.909039, train loss: 0.000533\n","batch: 110/391, train acc: 0.910815, train loss: 0.000576\n","batch: 120/391, train acc: 0.911034, train loss: 0.000624\n","batch: 130/391, train acc: 0.910931, train loss: 0.000678\n","batch: 140/391, train acc: 0.911204, train loss: 0.000728\n","batch: 150/391, train acc: 0.910275, train loss: 0.000785\n","batch: 160/391, train acc: 0.910987, train loss: 0.000833\n","batch: 170/391, train acc: 0.910648, train loss: 0.000886\n","batch: 180/391, train acc: 0.910125, train loss: 0.00094 \n","batch: 190/391, train acc: 0.909701, train loss: 0.000999\n","batch: 200/391, train acc: 0.90977 , train loss: 0.001047\n","batch: 210/391, train acc: 0.910411, train loss: 0.001092\n","batch: 220/391, train acc: 0.910642, train loss: 0.001141\n","batch: 230/391, train acc: 0.910994, train loss: 0.001187\n","batch: 240/391, train acc: 0.91101 , train loss: 0.00124 \n","batch: 250/391, train acc: 0.910792, train loss: 0.001294\n","batch: 260/391, train acc: 0.911126, train loss: 0.001341\n","batch: 270/391, train acc: 0.911209, train loss: 0.00139 \n","batch: 280/391, train acc: 0.911447, train loss: 0.00144 \n","batch: 290/391, train acc: 0.911151, train loss: 0.001496\n","batch: 300/391, train acc: 0.911139, train loss: 0.001548\n","batch: 310/391, train acc: 0.911085, train loss: 0.001601\n","batch: 320/391, train acc: 0.910982, train loss: 0.001656\n","batch: 330/391, train acc: 0.910587, train loss: 0.001712\n","batch: 340/391, train acc: 0.910421, train loss: 0.001768\n","batch: 350/391, train acc: 0.910414, train loss: 0.001822\n","batch: 360/391, train acc: 0.910257, train loss: 0.001876\n","batch: 370/391, train acc: 0.91062 , train loss: 0.001923\n","batch: 380/391, train acc: 0.910612, train loss: 0.001979\n","batch: 390/391, train acc: 0.910209, train loss: 0.00204 \n","epoch:   1, train acc: 0.910209, train loss: 0.00204 , test acc: 0.897108, test loss: 0.003077\n","Saved a model checkpoint at /content/drive/My Drive/cifar10/experiment/models/checkpoints/02_100-200_CIFAR10_ResNet18_epoch=1.pth\n","\n","batch:  10/391, train acc: 0.911114, train loss: 6e-05   \n","batch:  20/391, train acc: 0.912356, train loss: 0.000106\n","batch:  30/391, train acc: 0.910238, train loss: 0.000158\n","batch:  40/391, train acc: 0.910375, train loss: 0.000203\n","batch:  50/391, train acc: 0.908828, train loss: 0.000254\n","batch:  60/391, train acc: 0.910927, train loss: 0.000299\n","batch:  70/391, train acc: 0.910796, train loss: 0.000352\n","batch:  80/391, train acc: 0.911491, train loss: 0.000401\n","batch:  90/391, train acc: 0.911132, train loss: 0.000452\n","batch: 100/391, train acc: 0.911629, train loss: 0.000503\n","batch: 110/391, train acc: 0.911216, train loss: 0.000558\n","batch: 120/391, train acc: 0.910074, train loss: 0.000613\n","batch: 130/391, train acc: 0.90898 , train loss: 0.000674\n","batch: 140/391, train acc: 0.908996, train loss: 0.000725\n","batch: 150/391, train acc: 0.910174, train loss: 0.000772\n","batch: 160/391, train acc: 0.910195, train loss: 0.000821\n","batch: 170/391, train acc: 0.910505, train loss: 0.000869\n","batch: 180/391, train acc: 0.910924, train loss: 0.000919\n","batch: 190/391, train acc: 0.911088, train loss: 0.000968\n","batch: 200/391, train acc: 0.910624, train loss: 0.001023\n","batch: 210/391, train acc: 0.910605, train loss: 0.001074\n","batch: 220/391, train acc: 0.910339, train loss: 0.001132\n","batch: 230/391, train acc: 0.910598, train loss: 0.001179\n","batch: 240/391, train acc: 0.909873, train loss: 0.001241\n","batch: 250/391, train acc: 0.909788, train loss: 0.001291\n","batch: 260/391, train acc: 0.909682, train loss: 0.001345\n","batch: 270/391, train acc: 0.909488, train loss: 0.001401\n","batch: 280/391, train acc: 0.909566, train loss: 0.001451\n","batch: 290/391, train acc: 0.909665, train loss: 0.001506\n","batch: 300/391, train acc: 0.909653, train loss: 0.001557\n","batch: 310/391, train acc: 0.909491, train loss: 0.001613\n","batch: 320/391, train acc: 0.909631, train loss: 0.001664\n","batch: 330/391, train acc: 0.909798, train loss: 0.001715\n","batch: 340/391, train acc: 0.910082, train loss: 0.001761\n","batch: 350/391, train acc: 0.910094, train loss: 0.001814\n","batch: 360/391, train acc: 0.910339, train loss: 0.001863\n","batch: 370/391, train acc: 0.910085, train loss: 0.001918\n","batch: 380/391, train acc: 0.910166, train loss: 0.001972\n","batch: 390/391, train acc: 0.910047, train loss: 0.002027\n","epoch:   2, train acc: 0.910047, train loss: 0.002027, test acc: 0.892931, test loss: 0.003125\n","Saved a model checkpoint at /content/drive/My Drive/cifar10/experiment/models/checkpoints/02_100-200_CIFAR10_ResNet18_epoch=2.pth\n","\n","batch:  10/391, train acc: 0.920694, train loss: 5.3e-05 \n","batch:  20/391, train acc: 0.917967, train loss: 0.000103\n","batch:  30/391, train acc: 0.920018, train loss: 0.000148\n","batch:  40/391, train acc: 0.919128, train loss: 0.000194\n","batch:  50/391, train acc: 0.917841, train loss: 0.000243\n","batch:  60/391, train acc: 0.918446, train loss: 0.000289\n","batch:  70/391, train acc: 0.919337, train loss: 0.000331\n","batch:  80/391, train acc: 0.918248, train loss: 0.000382\n","batch:  90/391, train acc: 0.916338, train loss: 0.000438\n","batch: 100/391, train acc: 0.916494, train loss: 0.000485\n","batch: 110/391, train acc: 0.916522, train loss: 0.000532\n","batch: 120/391, train acc: 0.916352, train loss: 0.000584\n","batch: 130/391, train acc: 0.915648, train loss: 0.000634\n","batch: 140/391, train acc: 0.91514 , train loss: 0.000687\n","batch: 150/391, train acc: 0.91451 , train loss: 0.000742\n","batch: 160/391, train acc: 0.913572, train loss: 0.000801\n","batch: 170/391, train acc: 0.912994, train loss: 0.000851\n","batch: 180/391, train acc: 0.91299 , train loss: 0.000903\n","batch: 190/391, train acc: 0.912862, train loss: 0.000956\n","batch: 200/391, train acc: 0.913184, train loss: 0.001005\n","batch: 210/391, train acc: 0.913441, train loss: 0.001051\n","batch: 220/391, train acc: 0.913812, train loss: 0.001096\n","batch: 230/391, train acc: 0.913296, train loss: 0.001151\n","batch: 240/391, train acc: 0.913387, train loss: 0.001202\n","batch: 250/391, train acc: 0.913152, train loss: 0.001259\n","batch: 260/391, train acc: 0.912772, train loss: 0.001312\n","batch: 270/391, train acc: 0.912309, train loss: 0.00137 \n","batch: 280/391, train acc: 0.912033, train loss: 0.001425\n","batch: 290/391, train acc: 0.911587, train loss: 0.001486\n","batch: 300/391, train acc: 0.911423, train loss: 0.001541\n","batch: 310/391, train acc: 0.911008, train loss: 0.001595\n","batch: 320/391, train acc: 0.910703, train loss: 0.001653\n","batch: 330/391, train acc: 0.910399, train loss: 0.001706\n","batch: 340/391, train acc: 0.910136, train loss: 0.001767\n","batch: 350/391, train acc: 0.910342, train loss: 0.001816\n","batch: 360/391, train acc: 0.910505, train loss: 0.001868\n","batch: 370/391, train acc: 0.910435, train loss: 0.001917\n","batch: 380/391, train acc: 0.910697, train loss: 0.001964\n","batch: 390/391, train acc: 0.910816, train loss: 0.002016\n","epoch:   3, train acc: 0.910816, train loss: 0.002016, test acc: 0.902666, test loss: 0.002975\n","Saved a model checkpoint at /content/drive/My Drive/cifar10/experiment/models/checkpoints/02_100-200_CIFAR10_ResNet18_epoch=3.pth\n","\n","batch:  10/391, train acc: 0.923093, train loss: 4.9e-05 \n","batch:  20/391, train acc: 0.921152, train loss: 9.3e-05 \n","batch:  30/391, train acc: 0.918399, train loss: 0.000143\n","batch:  40/391, train acc: 0.920016, train loss: 0.000187\n","batch:  50/391, train acc: 0.919743, train loss: 0.000235\n","batch:  60/391, train acc: 0.919353, train loss: 0.000284\n","batch:  70/391, train acc: 0.917964, train loss: 0.000337\n","batch:  80/391, train acc: 0.917848, train loss: 0.000387\n","batch:  90/391, train acc: 0.918311, train loss: 0.00043 \n","batch: 100/391, train acc: 0.91797 , train loss: 0.000478\n","batch: 110/391, train acc: 0.917425, train loss: 0.000528\n","batch: 120/391, train acc: 0.917103, train loss: 0.000582\n","batch: 130/391, train acc: 0.916406, train loss: 0.000637\n","batch: 140/391, train acc: 0.915222, train loss: 0.000694\n","batch: 150/391, train acc: 0.914411, train loss: 0.000747\n","batch: 160/391, train acc: 0.915222, train loss: 0.000792\n","batch: 170/391, train acc: 0.915558, train loss: 0.000841\n","batch: 180/391, train acc: 0.915588, train loss: 0.000888\n","batch: 190/391, train acc: 0.915326, train loss: 0.00094 \n","batch: 200/391, train acc: 0.91524 , train loss: 0.00099 \n","batch: 210/391, train acc: 0.915411, train loss: 0.00104 \n","batch: 220/391, train acc: 0.91609 , train loss: 0.001082\n","batch: 230/391, train acc: 0.915092, train loss: 0.001142\n","batch: 240/391, train acc: 0.914695, train loss: 0.001198\n","batch: 250/391, train acc: 0.914329, train loss: 0.001253\n","batch: 260/391, train acc: 0.913869, train loss: 0.001309\n","batch: 270/391, train acc: 0.91376 , train loss: 0.001359\n","batch: 280/391, train acc: 0.913967, train loss: 0.001409\n","batch: 290/391, train acc: 0.91381 , train loss: 0.00146 \n","batch: 300/391, train acc: 0.913893, train loss: 0.001509\n","batch: 310/391, train acc: 0.914226, train loss: 0.001555\n","batch: 320/391, train acc: 0.914422, train loss: 0.001601\n","batch: 330/391, train acc: 0.914502, train loss: 0.001651\n","batch: 340/391, train acc: 0.914219, train loss: 0.001707\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"E0SCkt7e8HAR"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"pGqQ6O5YGCwu"},"source":[""],"execution_count":null,"outputs":[]}]}